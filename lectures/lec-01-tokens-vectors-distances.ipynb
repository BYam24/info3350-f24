{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFO 3350/6350\n",
    "\n",
    "## Lecture 01: Tokens, vectorization, and distance metrics\n",
    "\n",
    "## To do\n",
    "\n",
    "* Friday sections\n",
    "  * Be prepared to discuss the reading in depth (Healy)\n",
    "* No lecture next Monday (Labor Day)\n",
    "* Extra credit for good, consistent answers on Ed\n",
    "* Study groups are great for homeworks\n",
    "* Questions?\n",
    "\n",
    "## Let us compare three objects ...\n",
    "\n",
    "Volunteers?\n",
    "\n",
    "Which two objects are most similar? Which is the outlier? Why?\n",
    "  \n",
    "## Why would we want to do this?\n",
    "\n",
    "One goal of humanistic inquiry and of scientific research is to compare objects, so that we can gather them into types and compare any one object to others that we observe. Think of biological species or literary genres or historical eras. But how can we measure the difference or similarity between objects that are, after all, always necessarily individual and unique?\n",
    "\n",
    "* Measuring the *properties* of objects lets us compare those objects to one another.\n",
    "  * But ... *which* properties?\n",
    "  * Example: We might count words by type to compare gender and sentiment in novels.\n",
    " \n",
    "## What is a vector?\n",
    "\n",
    "An ordered collection of numbers that locate a point in space relative to a shared reference point (called the *origin*).\n",
    "\n",
    "* We can also think of vectors as representing the quantified *features* of an object.\n",
    "* Vectors are usually written as *row matrices*, or just as lists: $vec = [1.0, 0.5, 3.0, 1.2]$\n",
    "* Vectors have as many *dimensions* as there are features of the object to represent.\n",
    "  * The number of features to represent is a choice of the experiment. There is no correct choice, though some choices are better than others for a given purpose.\n",
    "* What is **vectorization**?\n",
    "  * The process of transforming an object into its vector representation, typically by measuring some of the object's properties.\n",
    "\n",
    "* Establishing a vector representation allows us to define a **distance metric** between objects that aren't straightforwardly spatial.\n",
    "  * \"Distance\" is a metaphor. Ditto \"similarity.\"\n",
    "  * Nothing is, in itself, like or unlike anything else. \n",
    "    * We sometimes seek to assert that objects are similar by erasing aspects of their particularity.\n",
    "  * Measuring similarity and difference are (always and only) interpretive interventions.\n",
    "  \n",
    "## A spatial example\n",
    "\n",
    "Consider this map of central campus:\n",
    "\n",
    "![](images/cornell_map.png)\n",
    "\n",
    "**How far apart are Gates Hall (purple star) and the clock tower (orange star)?**\n",
    "\n",
    "What do we need to know or define in order to answer this question?\n",
    "\n",
    "* Where is each building in physical space.\n",
    "  * Latitude/longitude; meters north/south and east/west of the book store; etc.\n",
    "* How do we want to measure the distance between them (walking, driving, flying, tunneling, ...). Minutes or miles?\n",
    "\n",
    "Normal, boring answer: about 0.4 miles on foot via Campus Rd and Ho Plaza, or a bit less if you cut some corners, or less than 0.3 miles if you can fly.\n",
    "\n",
    "| Clock tower | Gates Hall |\n",
    "| --- | --- | \n",
    "| ![](images/clock_tower.jpg) | ![](images/gates.jpg) |\n",
    "\n",
    "More interesting version: How far apart are these buildings conceptually? Architecturally? Historically? \n",
    "\n",
    "* What are the features and metrics you would use to answer this question?\n",
    "* This is a lot more like the problem of comparing texts.\n",
    "\n",
    "## A textual example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = '''\\\n",
    "My cat likes water.\n",
    "The dog eats food.\n",
    "The dog and the cat play together.\n",
    "A dog and a cat meet another dog and cat.\n",
    "The end.'''\n",
    "\n",
    "# Print with sentence numbers\n",
    "for line in enumerate(text.split('\\n')):\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us stipulate that we want to compare these five sentences according to their \"*dogness*\" and \"*catness*.\" We care about those two aspects alone, nothing else.\n",
    "\n",
    "Let's develop some intuitions here:\n",
    "\n",
    "* Sentences 0 and 1 are as far apart as can be: 0 is about cats, 1 is about dogs.\n",
    "* Sentence 2 lies between 0 and 1. It contains a mix of dogness and catness.\n",
    "* Sentence 3 is kind of like sentence 2, but it has twice as much of both dogness and catness.\n",
    "  * How different are sentences 2 and 3? (There's no objectively correct answer.)\n",
    "* Sentence 4 is a zero point. It has no dogness or catness.\n",
    "\n",
    "### Count relevant words\n",
    "\n",
    "||**cat**|**dog**|\n",
    "|---|---|---|\n",
    "|**sentence**| | |\n",
    "|0|1|0|\n",
    "|1|0|1|\n",
    "|2|1|1|\n",
    "|3|2|2|\n",
    "|4|0|0|\n",
    "\n",
    "The **vector representation** of sentence 0 is `[1, 0]`. The vector representation of sentence 3 is `[2, 2]`. And so on ...\n",
    "### Visualize (scatter plot)\n",
    "\n",
    "Sketch this by hand ...\n",
    "\n",
    "### Distance measures\n",
    "\n",
    "How far apart are sentences 0 and 1 (and all the rest)?\n",
    "\n",
    "#### Manhattan distance\n",
    "\n",
    "* Also called \"city block\" distance. \n",
    "* Not much used, but easy to understand and to compute (which matters for very large data sets). \n",
    "* Sum of the absolute difference in each dimension.\n",
    "\n",
    "For **sentences 0 and 1**, the Manhattan distance = |1| + |-1| = 2.\n",
    "\n",
    "#### Euclidean distance\n",
    "\n",
    "* Straight-line or \"as the crow flies\" distance. \n",
    "* Widely used in data science, but not always the best choice for textual data.\n",
    "\n",
    "Recall the Pythagorean theorem for the hypotenuse of a triangle: $a^2 = b^2 + c^2$ or $a = \\sqrt{b^2 +c^2}$.\n",
    "\n",
    "For **sentences 0 and 1**, the Euclidean distance = $\\sqrt{1^2 + 1^2} = \\sqrt{2} = 1.414$.\n",
    "\n",
    "OK, but what about the Euclidean distance between **sentence 0 and sentence 3**? Well, that distance = $\\sqrt{1^2 + 2^2} = \\sqrt{5} = 2.24$.\n",
    "\n",
    "And between **sentences 2 and 3** (both balanced 50:50 between dogs and cats)? That's 1.4 again, the same as the distance between sentences 0 and 1 (which, recall, are totally divergent in dog/cat content).\n",
    "\n",
    "An obvious improvement in this case would be to **normalize word counts by document length**.\n",
    "\n",
    "#### Cosine distance\n",
    "\n",
    "Maybe instead of distance, we could measure the difference in **direction** from the origin between points.\n",
    "\n",
    "* **Sentences 0 and 1** are 90 degrees apart.\n",
    "* **Sentences 2 and 3** are 0 degrees apart.\n",
    "* **Sentences 0 and 1** are each 45 degrees away from **sentences 2 and 3**.\n",
    "\n",
    "Now, recall the values of the **cosine** of an angle between 0 and 90 degrees. (Sketch by hand)\n",
    "\n",
    "So, the cosines of the angles between sentences are:\n",
    "\n",
    "sentences|angle|cosine\n",
    "---|---|---\n",
    "0 and 1|90|0\n",
    "2 and 3|0|1\n",
    "0 and 2|45|0.707\n",
    "0 and 3|45|0.707\n",
    "1 and 2|45|0.707\n",
    "\n",
    "We could then transform these cosine **similarities** into **distances** by subtracting them from 1, so that the most *dissimilar* sentences (like 0 and 1) have the greatest distance between them.\n",
    "\n",
    "The big advantage here is that we don't need to worry about getting length normalization right. Cosine distance is often a good choice for text similarity tasks.\n",
    "\n",
    "#### Higher dimensions\n",
    "\n",
    "All of these metrics can be calculated in arbitrarily many dimensions. Which is good, because textual data is often very high-dimensional. Imagine counting the occurrences of each word type in a large corpus of novels or historical documents. Can easily be tens of thousands of dimensions.\n",
    "\n",
    "## In the real world\n",
    "\n",
    "* There's nothing wrong with any of these vectorizations and distance metrics, exactly, but they're not state of the art.\n",
    "* If you've done some recent NLP work, you'll know that, at the very least, you'd want to use static word embeddings in place of raw tokens.\n",
    "  * This allows you to capture the similarity of meaning between, e.g., \"cat\" and \"kitten.\"\n",
    "  * Word counts alone represent any two distinct word types as (entirely) separate dimensions, so \"cat\" and \"kitten\" have the same inherent relationship (none) as \"cat\" and \"dog\" or \"cat\" and \"algebraic\".\n",
    "* If you were especially ambitious, you'd be looking at something like BERT or ELMo or GPT-*, etc.\n",
    "    * These transformer-based methods allow for *contextual* embeddings, that is, they represent a word token differently depending on the context in which it appears, so that the representation of \"bank\" in \"my money is in the bank\" is different from the the representation of \"bank\" in \"we walked along the bank of the river.\"\n",
    "* We'll cover both static and contextual embeddings later this semester.\n",
    "* And then you might want features that correspond to aspects of a text other than the specific words it contains.\n",
    "    * When was it written?\n",
    "    * By *whom* was it written?\n",
    "    * How long is it?\n",
    "    * In what style is it written?\n",
    "    * Who read it?\n",
    "    * How much did it cost?\n",
    "    * How many people read or reviewed it?\n",
    "    * What else did its readers also read?\n",
    "    * And so on ...\n",
    "\n",
    "Here, though, we're trying to grasp the *idea* behind document similarity, on which all of these methods depend: transform text into a numeric representation of its features (often, a representation of its content or meaning), then quantify the difference or similarity between those numeric representations.\n",
    "\n",
    "## In the problem set world\n",
    "\n",
    "We'll dig into how, as a practical matter, we can vectorize texts and calclulate distance metrics in this week's problem set.\n",
    "\n",
    "We'll use `scikit-learn` to implement vectorization and distance metrics. The `scikit-learn` API almost always involves *three* steps:\n",
    "\n",
    "1. Instantiate a learning object (such as a vectorizer, regressor, classifier, etc.). This is the object that will hold the parameters of your fitted model.\n",
    "1. Call the instantiated learning object's `.fit()` method, passing in your data. This allows the model to learn the optimal parameters from your data.\n",
    "1. Call the fitted model's `.transform()` or `.predict()` method, passing in either the same data from the `fit` step or new data. This step uses the fitted model to generate outputs given the input data you supply.\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# get example text as one doc per line\n",
    "docs = [sent for sent in text.split('\\n')]\n",
    "\n",
    "# instantiate vectorizer object\n",
    "#  note setup options\n",
    "vectorizer = CountVectorizer(\n",
    "    vocabulary=['cat', 'dog']\n",
    ")\n",
    "\n",
    "# fit to data\n",
    "vectorizer.fit(docs)\n",
    "\n",
    "# transform docs to features\n",
    "features = vectorizer.transform(docs)\n",
    "\n",
    "# print output feature matrix\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(features.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# calculate distances\n",
    "from sklearn.metrics.pairwise import euclidean_distances, cosine_distances, cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "print(\"Euclidean distances\")\n",
    "print(np.round(euclidean_distances(features),2))\n",
    "\n",
    "print(\"\\nCosine distances\")\n",
    "print(np.round(cosine_distances(features),2))\n",
    "\n",
    "print(\"\\nCosine **similarities**\")\n",
    "print(np.round(cosine_similarity(features),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# FYI, two heatmap visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"Euclidean distances\")\n",
    "\n",
    "# quick and dirty\n",
    "plt.imshow(euclidean_distances(features))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# prettier\n",
    "sns.heatmap(\n",
    "    euclidean_distances(features),\n",
    "    annot=True,\n",
    "    square=True\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization considerations\n",
    "\n",
    "What is a token?\n",
    "\n",
    "* The **smallest individually meaningful unit of a document.** Roughly, a word.\n",
    "* But ... as soon as you see \"meaningful,\" you know it's going to be a matter of interpretation.\n",
    "  * *Every single thing you do in text analysis is an interpretive intervention!*\n",
    "* Not all tokens are (single) words. For example:\n",
    "  * **Contractions**. `\"I'm\"` or `\"can't\"`. One token or two?\n",
    "  * **Phrases.** `\"San Francisco\"` or `\"Cornell University\"`. Two tokens or one?\n",
    "    * These are exampled of \"named entities.\" We'll revisit them later in the semester.\n",
    "  * **Punctuation.** Count it at all? Is `\"this\"` the same token as `\"this!\"`? Is `\".\"` or `\";\"` a token on its own?\n",
    "  * **Domain-specific terms.** `\"@user\"`, `\"COVID-19\"`, etc.\n",
    "\n",
    "Tokenization is part of the more-or-less standard text-processing workflow. Other parts of that workflow might include:\n",
    "  * Case regularization/folding\n",
    "  * Punctuation removal\n",
    "  * Lemmatization or stemming\n",
    "  * Sentence segmentation\n",
    "  * and more ...\n",
    "  \n",
    "## State of the art\n",
    "\n",
    "A decade ago, using raw tokens for NLP tasks was the best we could do. Today, we generally use static or contextual word *embeddings* in place of tokens. We'll talk about this at length in the second half of the course, but the underlying idea is the same. Words and embeddings are proxies for meaning (which is what we ultimately care about, but is never directly accessible to us). Embeddings are just a way to capture more of the specific meaning of a word as it is used in a given language (static) or linguistic context (contextual).\n",
    "\n",
    "## Tokenization can be domain-specific\n",
    "\n",
    "Note that today's reading assumed some special interests:\n",
    "\n",
    "* Twitter(like) texts\n",
    "* Sentiment as target phenomenon\n",
    "\n",
    "So it worked hard to capture Twitter handles, hashtags, smilies, URLs, etc.\n",
    "\n",
    "The \"right\" way to tokenize depends on your project, on what is meaningful *in context*.\n",
    "If you have different data or different phenomena to investigate, you might tokenize differently.\n",
    "\n",
    "## Approach 1: Split on whitespace\n",
    "\n",
    "A simple, naïve approach, workable for quick-and-dirty work with many Western languages.\n",
    "\n",
    "Consider the sentence:\n",
    "\n",
    "> Cornell is a private, Ivy League university and the land-grant university for New York state.\n",
    "\n",
    "How many tokens does this sentence contain? (count them for yourself)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cornell = 'Cornell is a private, Ivy League university and the land-grant university for New York state.'\n",
    "tokens = cornell.split()\n",
    "print(tokens)\n",
    "print(\"Number of tokens:\", len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice: `private,` `land-grant` `state.` These aren't wrong *per se*, but ...\n",
    "\n",
    "Maybe we could do better if we just took non-space, non-puctuation strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "word_pattern = re.compile(\"[\\w]+\")\n",
    "tokens_re = word_pattern.findall(cornell)\n",
    "print(tokens_re)\n",
    "print(\"Number of tokens:\", len(tokens_re))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular expressions\n",
    "\n",
    "A totally inadequate mini-introduction to an important but annoyingly complex technology.\n",
    "\n",
    "* What is a regular expression (regex)?\n",
    "  * A sequence of characters that define a search pattern.\n",
    "  * That is, it's a text search or matching language.\n",
    "  * Notoriously unreadable and difficult to parse by eye.\n",
    "  \n",
    "Consider the line above:\n",
    "\n",
    "```\n",
    "word_pattern = re.compile(\"[\\w]+\")\n",
    "```\n",
    "\n",
    "The search pattern here is any sequence of one or more (`+`) uniterrupted \"word\" characters (`\\w` = upper- and lowercase letters, plus digits) that occur anywhere in a string. Regexes are usually \"greedy,\" so will continue matching character by character until their condition is not met."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in ['t', 'the', 'these', \"these'uns\", \"these ones\"]:\n",
    "    print(word_pattern.findall(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`re` is Python's regular expression library. `compile` prepares the regular expression for use with text inputs.\n",
    "\n",
    "A few other useful bits of regex syntax:\n",
    "\n",
    "* `.` (period) = any character\n",
    "* `\\s` = whitespace character (space, tab, newline, etc.)\n",
    "* `\\d` = digit\n",
    "* `[abc]` = any character in the set {a, b, c}.\n",
    "* `[^abc]` = negation, any character *except* a, b, or c.\n",
    "* `A*` = zero or more occurrences of the character A; `+` = one or more, `?` = zero or one.\n",
    "* `\\A`, `\\Z`, `^`, and `$` = match only at start or end of a string or line, respectively.\n",
    "* `\\` (backslash) = escape the next character; `\\.` = period, not wildcard.\n",
    "\n",
    "There's a lot more to this. Take a look at the code linked from today's reading, and/or consult a [regex cheat sheet](https://learnbyexample.github.io/cheatsheet/python/python-regex-cheatsheet/).\n",
    "\n",
    "Why use regular expressions?\n",
    "  * A powerful way to find/match/extract substrings from strings and texts.\n",
    "  * Can use regexes to build robust custom tokenizers (as in the reading for today)\n",
    "\n",
    "### NLTK\n",
    "\n",
    "The Natural Language Tool Kit (NLTK) is a full-featured Python NLP library. It includes a bunch of tokenizers, nearly all of them extensible, that will probably perform better than whatever you can hack together for your project.\n",
    "\n",
    "Let's try it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "tokens_nltk = word_tokenize(cornell)\n",
    "print(tokens_nltk)\n",
    "print('Number of tokens:', len(tokens_nltk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokenize(\"can't, I'm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that NLTK treats word-terminal punctuation as a token and is smart about contractions.\n",
    "\n",
    "## Non-English/Non-Western text\n",
    "\n",
    "Whitespace can be a very bad approach if Western typographic conventions don't apply!\n",
    "\n",
    "If you don't know the language:\n",
    "\n",
    "* Ask if you should be doing the work\n",
    "* Lean on libraries\n",
    "\n",
    "### Example from the *New York Times*\n",
    "\n",
    "In a [recent *Times* article](https://www.nytimes.com/2020/09/03/sports/soccer/premier-league-china-contract-television.html) on football broadcasting rights, we find this sentence:\n",
    "\n",
    "**Chinese**\n",
    "\n",
    "> 因受新型冠状病毒危机对足球和其他体育赛事的持续影响，早已面临越来越多亏损的英格兰超级足球联赛周四宣布，因为无法解决与中国合作伙伴的纠纷，已终止了其最赚钱的海外转播合同。\n",
    "\n",
    "**English translation**\n",
    "\n",
    "> The English Premier League, already facing mounting losses because of the continued impact of the coronavirus crisis on soccer and other sporting events, announced on Thursday that it had canceled its most lucrative overseas broadcast contract after it was unable to resolve a dispute with its Chinese partner.\n",
    "\n",
    "Our previous tokenization strategy doesn't work well in this case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strings\n",
    "zh = '因受新型冠状病毒危机对足球和其他体育赛事的持续影响，早已面临越来越多亏损的英格兰超级足球联赛周四宣布，因为无法解决与中国合作伙伴的纠纷，已终止了其最赚钱的海外转播合同。'\n",
    "en = 'The English Premier League, already facing mounting losses because of the continued impact of the coronavirus crisis on soccer and other sporting events, announced on Thursday that it had canceled its most lucrative overseas broadcast contract after it was unable to resolve a dispute with its Chinese partner.'\n",
    "\n",
    "# Naive approach to tokenization\n",
    "zh_tokens_bad = zh.split()\n",
    "print(zh_tokens_bad)\n",
    "print('Number of Chinese tokens:', len(zh_tokens_bad))\n",
    "\n",
    "# English version\n",
    "en_tokens = en.split()\n",
    "print('Number of English tokens:', len(en_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `jieba` tokenizer\n",
    "\n",
    "See the [`jieba` project GitHub page](https://github.com/fxsjy/jieba) for documentation (in Chinese and in English). `jieba` is one of the packages we installed in our virtual environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A better approach to tokenizing Chinese-language text\n",
    "import jieba\n",
    "zh_tokens_better = [token for token in jieba.cut(zh)]\n",
    "print(zh_tokens_better)\n",
    "print(\"Number of Chinese tokens:\", len(zh_tokens_better))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
